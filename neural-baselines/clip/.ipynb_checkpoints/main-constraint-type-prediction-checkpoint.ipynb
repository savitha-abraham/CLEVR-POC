{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1be92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: xxhash in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (0.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (0.8.1)\n",
      "Requirement already satisfied: packaging in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: importlib_metadata in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from datasets) (4.12.0)\n",
      "Requirement already satisfied: filelock in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from importlib_metadata->datasets) (3.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (4.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: filelock in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from requests->transformers) (1.26.10)\n",
      "Requirement already satisfied: pandas in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (0.7.0+cpu)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: torch==1.6.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from torchvision) (1.6.0+cpu)\n",
      "Requirement already satisfied: numpy in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: future in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from torch==1.6.0->torchvision) (0.18.2)\n",
      "Requirement already satisfied: albumentations in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from albumentations) (4.6.0.66)\n",
      "Requirement already satisfied: scikit-image<0.19,>=0.16.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from albumentations) (0.18.3)\n",
      "Requirement already satisfied: PyYAML in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from albumentations) (6.0)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: scipy in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from albumentations) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from albumentations) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (4.1.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from scikit-image<0.19,>=0.16.1->albumentations) (2021.11.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from scikit-image<0.19,>=0.16.1->albumentations) (1.3.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from scikit-image<0.19,>=0.16.1->albumentations) (2.6.3)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from scikit-image<0.19,>=0.16.1->albumentations) (3.5.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from scikit-image<0.19,>=0.16.1->albumentations) (2.19.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from scikit-image<0.19,>=0.16.1->albumentations) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations) (4.34.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations) (2.8.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (1.6.0+cpu)\n",
      "Requirement already satisfied: torchvision in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (0.7.0+cpu)\n",
      "Requirement already satisfied: torchaudio in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from torch) (1.21.5)\n",
      "Requirement already satisfied: future in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: ipdb in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (0.13.9)\n",
      "Requirement already satisfied: toml>=0.10.2 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipdb) (0.10.2)\n",
      "Requirement already satisfied: ipython>=7.17.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipdb) (7.34.0)\n",
      "Requirement already satisfied: setuptools in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipdb) (61.2.0)\n",
      "Requirement already satisfied: decorator in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipdb) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (3.0.30)\n",
      "Requirement already satisfied: pygments in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (2.12.0)\n",
      "Requirement already satisfied: backcall in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (5.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install opencv\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip install torchvision\n",
    "!pip install albumentations\n",
    "#!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install ipdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7c4106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms \n",
    "from torchvision import models as models\n",
    "\n",
    "import pandas\n",
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import CLIPModel, CLIPProcessor, AutoModel, AutoConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from datasets import load_dataset, load_metric, DownloadConfig, load_from_disk, DatasetDict\n",
    "\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140e121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## CONFIGURATION\n",
    "\n",
    "data_folder = os.path.expanduser('~/code/clevr-poc/data')\n",
    "constraint_types_tensor_file_path = os.path.join(data_folder, 'constraint_types_tensor.pickle')\n",
    "properties_file_path = os.path.join(data_folder, 'properties.json')\n",
    "\n",
    "model_path = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 8\n",
    "num_workers = 8\n",
    "pin_memory=8\n",
    "gradient_accumulation=4\n",
    "epochs = 20\n",
    "\n",
    "max_length = 42\n",
    "\n",
    "dropout = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12934df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of classes\n",
    "with open (properties_file_path, 'rb') as f:\n",
    "    properties = json.load(f)\n",
    "\n",
    "total_number_of_classes = 0\n",
    "for key, value in properties.items():\n",
    "    total_number_of_classes =  total_number_of_classes + len(value.keys())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a50cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# get constraints tensors\n",
    "\n",
    "with open (constraint_types_tensor_file_path, 'rb') as f:\n",
    "    constraint_types_tensor = pickle.load(f)\n",
    "num_constraint_types = len(constraint_types_tensor)\n",
    "\n",
    "print(num_constraint_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a9fd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageConstraintTypeClassification(nn.Module):\n",
    "    def __init__(self, device, input_dim=2048, output_dim=None):\n",
    "        super(ImageConstraintTypeClassification,self).__init__()    \n",
    "        self.device = device\n",
    "        self.model = models.resnet50(progress=True, pretrained=True)\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.model.fc = nn.Linear(2048, num_constraint_types)\n",
    "        #self.model = nn.Linear(input_dim, output_dim)\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "class ClipClassification(nn.Module):\n",
    "    def __init__(self, \n",
    "                 device, \n",
    "                 checkpoint, \n",
    "                 clip_embedding_size,\n",
    "                 ctype_embedding_size,\n",
    "                 output_dim,\n",
    "                 num_constraint_types):\n",
    "        \n",
    "        super(ClipClassification,self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "\n",
    "        #self.ctype_classifier = ImageConstraintTypeClassification(device, input_dim=clip_embedding_size, output_dim=num_constraint_types)\n",
    "        self.ctype_classifier = ImageConstraintTypeClassification(device=device, output_dim=num_constraint_types)\n",
    "        \n",
    "        self.clip_model = CLIPModel.from_pretrained(checkpoint)\n",
    "        #for param in self.clip_model.parameters():\n",
    "        #    param.requires_grad = False\n",
    "        self.clip_model.to(self.device)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        input_dim = clip_embedding_size*2 + ctype_embedding_size\n",
    "        self.classifier = nn.Linear(input_dim, self.output_dim) # load and initialize weights\n",
    "        self.classifier.to(self.device)\n",
    "\n",
    "\n",
    "    # define a function that returns the tensor of a specific constraint type\n",
    "    def get_tensor(constraint_type):\n",
    "        return torch.flatten(constraint_types_tensor[constraint_type])\n",
    "   \n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, pixel_values=None, labels=None, constraint_type=None, image=None):\n",
    "    #def forward(self, input_ids=None, attention_mask=None, pixel_values=None, labels=None):\n",
    "               \n",
    "        outputs = self.clip_model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        text_emb = outputs['text_embeds']    #batchx512\n",
    "        image_emb = outputs['image_embeds']  #batchx512\n",
    "               \n",
    "        ctype_classification_output = self.ctype_classifier(image)\n",
    "        v, predicted_constraint_type = torch.max(ctype_classification_output, 1)\n",
    "                \n",
    "        \n",
    "        #constraint_type_list = constraint_type.tolist()\n",
    "        constraint_type_list = predicted_constraint_type.tolist()\n",
    "        constraint_type_embedding = list(map(ClipClassification.get_tensor, constraint_type_list))\n",
    "        constraint_type_embedding = torch.stack([x for x in constraint_type_embedding], dim=0).to(self.device)\n",
    "        #Add custom layers\n",
    "   \n",
    "        emb = torch.cat([text_emb,image_emb,constraint_type_embedding], dim=1)  \n",
    "        \n",
    "        emb = self.dropout(emb)\n",
    "\n",
    "\n",
    "        logits = self.classifier(emb) # calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.output_dim), labels.view(-1))\n",
    "\n",
    "        hidden = outputs['text_model_output']['last_hidden_state']\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits, hidden_states=hidden,attentions=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d1867d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 10:20:34,293 [INFO] Loading dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/tmp/ipykernel_4895/1818158740.py\u001b[0m(6)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      5 \u001b[0;31m\u001b[0mdl_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 6 \u001b[0;31m\u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading training data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 10:20:36,020 [INFO] Loading training data\n",
      "2022-08-09 10:20:36,075 [WARNING] Reusing dataset clevr-poc-loader (/home/marjan/.cache/huggingface/datasets/clevr-poc-loader/clevr-poc/1.1.0/3a4e5e384b243c25aa4faebb0c31993f11e2c2ff068543ca7c5195670276d0bf)\n",
      "2022-08-09 10:20:36,085 [INFO] Loading validation data\n",
      "2022-08-09 10:20:36,100 [WARNING] Reusing dataset clevr-poc-loader (/home/marjan/.cache/huggingface/datasets/clevr-poc-loader/clevr-poc/1.1.0/3a4e5e384b243c25aa4faebb0c31993f11e2c2ff068543ca7c5195670276d0bf)\n",
      "2022-08-09 10:20:36,103 [INFO] Loading test data\n",
      "2022-08-09 10:20:36,118 [WARNING] Reusing dataset clevr-poc-loader (/home/marjan/.cache/huggingface/datasets/clevr-poc-loader/clevr-poc/1.1.0/3a4e5e384b243c25aa4faebb0c31993f11e2c2ff068543ca7c5195670276d0bf)\n",
      "2022-08-09 10:20:36,120 [INFO] Dataset loaded\n",
      "2022-08-09 10:20:36,121 [INFO] Loading CLIP\n",
      "2022-08-09 10:20:44,161 [INFO] Transforming dataset\n",
      "2022-08-09 10:20:44,164 [WARNING] Parameter 'function'=<function transform_tokenize at 0x7f3aa2b0cf80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|█████████████████████████████████████████████| 3/3 [03:34<00:00, 71.50s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:58<00:00, 58.57s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [01:00<00:00, 60.91s/ba]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\", handlers=[logging.StreamHandler()])\n",
    "\n",
    "logging.info(\"Loading dataset\")\n",
    "\n",
    "dl_config = DownloadConfig(resume_download=True, num_proc=4)\n",
    "import ipdb; ipdb.set_trace()\n",
    "logging.info('Loading training data')\n",
    "dataset_train = load_dataset('clevr-poc-loader.py',\n",
    "                       name='clevr-poc',\n",
    "                       download_config=dl_config,\n",
    "                       split='train[:]')\n",
    "logging.info('Loading validation data')\n",
    "dataset_val = load_dataset('clevr-poc-loader.py',\n",
    "                       name='clevr-poc',\n",
    "                       download_config=dl_config,\n",
    "                       split='validation[:]')\n",
    "logging.info('Loading test data')\n",
    "dataset_test = load_dataset('clevr-poc-loader.py',\n",
    "                       name='clevr-poc',\n",
    "                       download_config=dl_config,\n",
    "                       split='test[:]')\n",
    "\n",
    "logging.info('Dataset loaded')\n",
    "\n",
    "dataset = DatasetDict({\n",
    "  'train':dataset_train,\n",
    "  'validation':dataset_val,\n",
    "  'test':dataset_test\n",
    "})\n",
    "\n",
    "logging.info('Loading CLIP')\n",
    "model_path = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "#TODO convert CLEVR images offline\n",
    "extractor = CLIPProcessor.from_pretrained(model_path)\n",
    "\n",
    "def image_traform(e):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    return convert_tensor(e)    \n",
    "\n",
    "def transform_tokenize(e):\n",
    "    e['image'] = [image.convert('RGB') for image in e['image']]\n",
    "    \n",
    "    \"\"\"\n",
    "    return extractor(text=e['question'],\n",
    "                               images=e['image'],\n",
    "                               truncation=True, \n",
    "                               #padding=True)\n",
    "                               padding=\"max_length\", max_length=42)\n",
    "    \"\"\"\n",
    "    \n",
    "    token = extractor(text=e['question'],\n",
    "                               images=e['image'],\n",
    "                               truncation=True, \n",
    "                               #padding=True)\n",
    "                               padding=\"max_length\", max_length=42)\n",
    "    \n",
    "    token['image'] = list(map(image_traform, e['image']))\n",
    "    return token\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "logging.info('Transforming dataset')\n",
    "dataset = dataset.map(transform_tokenize, batched=True, num_proc=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e912d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('accuracy')\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits[:-1], axis=-1)[0]\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "clip_embedding_size = 512\n",
    "ctype_embedding_size = constraint_types_tensor[0].shape[0]*constraint_types_tensor[0].shape[1]\n",
    "model = ClipClassification(device=device, \n",
    "                           checkpoint=model_path,\n",
    "                           clip_embedding_size = clip_embedding_size,\n",
    "                           ctype_embedding_size = ctype_embedding_size,\n",
    "                           output_dim=total_number_of_classes, \n",
    "                           num_constraint_types=num_constraint_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7c0747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 10:26:26,587 [INFO] Creating trainer\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Creating trainer\")\n",
    "training_args = TrainingArguments(\"test_trainer\",\n",
    "                                    num_train_epochs=epochs,\n",
    "                                    per_device_train_batch_size=train_batch_size,\n",
    "                                    per_device_eval_batch_size=eval_batch_size,\n",
    "                                    fp16=True if device == 'cuda' else False,\n",
    "                                    dataloader_num_workers=num_workers ,\n",
    "                                    dataloader_pin_memory=pin_memory,\n",
    "                                    gradient_accumulation_steps=gradient_accumulation,\n",
    "                                    save_strategy='no',\n",
    "                                    evaluation_strategy='epoch',\n",
    "                                    eval_steps=1)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7187c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 10:26:26,617 [INFO] Training model\n",
      "The following columns in the training set  don't have a corresponding argument in `ClipClassification.forward` and have been ignored: question. If question are not expected by `ClipClassification.forward`,  you can safely ignore this message.\n",
      "/home/marjan/anaconda3/envs/clip/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='1860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  54/1860 14:45 < 8:32:35, 0.06 it/s, Epoch 0.57/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.info(\"Training model\")\n",
    "training_metrics = trainer.train()\n",
    "logging.info(training_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, test_metrics = trainer.predict(dataset['test'])\n",
    "y_true = dataset['test']['label']                                                                                                                 \n",
    "y_pred = np.argmax(predictions[:-1], axis=-1)[0]                                                                                                    \n",
    "confusion_matrix = metrics.confusion_matrix(y_true, y_pred, labels=[i for i in range(total_number_of_classes)])                                                                                                            \n",
    "print(confusion_matrix)\n",
    "logging.info(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa390366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
